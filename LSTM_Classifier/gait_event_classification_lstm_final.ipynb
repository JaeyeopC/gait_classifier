{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e9afb02-c70f-48c7-9581-9bd458ac1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f4f991-d89e-4bdb-9238-f4c13443d25e",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing Functions\n",
    "\n",
    "In this section, we define two essential functions for handling our dataset: `load_data_from_files` and `preprocess_data`. These functions streamline the process of loading raw data from CSV files and preparing it for analysis or modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## `load_data_from_files(data_dir)`\n",
    "\n",
    "### Purpose\n",
    "This function is responsible for loading all CSV files from a specified directory, extracting relevant metadata from each filename, and consolidating the data into a single pandas DataFrame.\n",
    "\n",
    "### Parameters\n",
    "- **`data_dir`** (`str`): The directory path where the CSV data files are stored.\n",
    "\n",
    "### Process\n",
    "1. **Initialize Storage:**\n",
    "   - An empty list `all_data` is created to hold individual DataFrames from each CSV file.\n",
    "\n",
    "2. **Iterate Through Files:**\n",
    "   - The function loops through each file in the `data_dir`.\n",
    "   - It filters out files that do not end with the `.csv` extension to ensure only relevant data files are processed.\n",
    "\n",
    "3. **Extract Metadata from Filename:**\n",
    "   - Filenames are expected to follow a specific format, allowing extraction of metadata:\n",
    "     - **`collection_id`**: Identifies the collection batch.\n",
    "     - **`step_info`**: Contains information about the step number and foot side.\n",
    "   - **`step_number`** is extracted by filtering digits from `step_info`.\n",
    "   - **`foot`** is determined based on the presence of 'R' (Right) or 'L' (Left) in `step_info`.\n",
    "\n",
    "4. **Load and Augment Data:**\n",
    "   - Each CSV file is read into a pandas DataFrame `df`.\n",
    "   - New columns are added to `df` to include the extracted metadata:\n",
    "     - `collection_id`\n",
    "     - `step_number` (converted to integer)\n",
    "     - `foot`\n",
    "     - `filename` (to keep track of the source file)\n",
    "\n",
    "5. **Aggregate Data:**\n",
    "   - The augmented DataFrame `df` is appended to the `all_data` list.\n",
    "\n",
    "6. **Concatenate All Data:**\n",
    "   - After processing all files, `all_data` is concatenated into a single DataFrame using `pd.concat`, with the index reset for consistency.\n",
    "\n",
    "### Returns\n",
    "- **`pd.DataFrame`**: A consolidated DataFrame containing data from all CSV files, enriched with metadata.\n",
    "\n",
    "---\n",
    "\n",
    "## `preprocess_data(data)`\n",
    "\n",
    "### Purpose\n",
    "This function preprocesses the loaded data to make it suitable for machine learning models. It handles missing values, normalizes feature scales, and encodes categorical target labels.\n",
    "\n",
    "### Parameters\n",
    "- **`data`** (`pd.DataFrame`): The raw DataFrame containing sensor data and metadata.\n",
    "\n",
    "### Process\n",
    "1. **Define Features and Target:**\n",
    "   - **`features`**: A list of sensor measurement columns:\n",
    "     - `'gyroscope_x'`, `'gyroscope_y'`, `'gyroscope_z'`\n",
    "     - `'accelerometer_x'`, `'accelerometer_y'`, `'accelerometer_z'`\n",
    "   - **`target`**: The label column `'phase'` that the model will predict.\n",
    "\n",
    "2. **Handle Missing Values:**\n",
    "   - Rows with missing (`NaN`) values in any of the `features` or the `target` are removed using `dropna`. This ensures data integrity for subsequent processing steps.\n",
    "\n",
    "3. **Normalize Features:**\n",
    "   - A `MinMaxScaler` is instantiated to scale feature values to a range between 0 and 1.\n",
    "   - The scaler is fitted to the `features` and used to transform the data, ensuring that all features contribute equally to the model training.\n",
    "\n",
    "4. **Encode Target Labels:**\n",
    "   - A `LabelEncoder` is used to convert categorical target labels in the `'phase'` column into numerical values.\n",
    "   - This encoding is essential for algorithms that require numerical input for the target variable.\n",
    "\n",
    "### Returns\n",
    "- **`data`** (`pd.DataFrame`): The preprocessed DataFrame with normalized features and encoded target labels.\n",
    "- **`label_encoder`** (`LabelEncoder`): The fitted label encoder instance, useful for inverse transforming predictions back to original labels.\n",
    "\n",
    "---\n",
    "\n",
    "By utilizing these functions, we ensure that our data is systematically loaded and preprocessed, paving the way for effective analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94500e7d-bb7d-4e81-a727-d38834260beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Data Loading and Preprocessing Functions #\n",
    "############################################\n",
    "\n",
    "def load_data_from_files(data_dir):\n",
    "    all_data = []\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Extract metadata from filename\n",
    "            collection_id, step_info, _ = filename.split('_', 2)\n",
    "            step_number = ''.join(filter(str.isdigit, step_info))\n",
    "            foot = 'R' if 'R' in step_info else 'L'\n",
    "            filepath = os.path.join(data_dir, filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "            df['collection_id'] = collection_id\n",
    "            df['step_number'] = int(step_number)\n",
    "            df['foot'] = foot\n",
    "            df['filename'] = filename  # Keep track of the file\n",
    "            all_data.append(df)\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Define features and target\n",
    "    features = ['gyroscope_x', 'gyroscope_y', 'gyroscope_z',\n",
    "                'accelerometer_x', 'accelerometer_y', 'accelerometer_z']\n",
    "    target = 'phase'\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    data = data.dropna(subset=features + [target])\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = MinMaxScaler()\n",
    "    data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "    # Encode target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    data[target] = label_encoder.fit_transform(data[target])\n",
    "\n",
    "    return data, label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c614a2f9-a004-4292-b45d-0eca8f28b48f",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader Preparation\n",
    "\n",
    "In this section, we define the `SensorDataset` class, a custom dataset tailored for handling time-series sensor data. This class is essential for preparing our data in a format suitable for training machine learning models, particularly those that operate on sequential data, such as Recurrent Neural Networks (RNNs) or Temporal Convolutional Networks (TCNs).\n",
    "\n",
    "---\n",
    "\n",
    "## `SensorDataset` Class\n",
    "\n",
    "### Purpose\n",
    "The `SensorDataset` class inherits from PyTorch's `Dataset` and is designed to:\n",
    "- Organize sensor data into sequences of a specified length.\n",
    "- Associate each sequence with the corresponding target label.\n",
    "- Facilitate efficient data loading during model training and evaluation.\n",
    "\n",
    "### Initialization (`__init__`)\n",
    "\n",
    "#### Parameters\n",
    "- **`data`** (`pd.DataFrame`): The preprocessed DataFrame containing sensor measurements and metadata.\n",
    "- **`features`** (`list` of `str`): List of feature column names to be used as input for the model (e.g., sensor readings).\n",
    "- **`target`** (`str`): The name of the target column that the model aims to predict (e.g., `'phase'`).\n",
    "- **`sequence_length`** (`int`): The number of consecutive time steps to include in each input sequence.\n",
    "\n",
    "#### Process\n",
    "1. **Attribute Assignment:**\n",
    "   - Stores the provided `features`, `target`, and `sequence_length` as instance attributes for later use.\n",
    "\n",
    "2. **Initialization of Storage Lists:**\n",
    "   - `self.sequences`: A list to store the input sequences.\n",
    "   - `self.labels`: A list to store the corresponding target labels for each sequence.\n",
    "\n",
    "3. **Grouping Data by Filename:**\n",
    "   - The data is grouped by the `'filename'` column to ensure that sequences are generated from contiguous data points within the same file, preventing mixing data from different sources.\n",
    "\n",
    "4. **Sequence Generation:**\n",
    "   - For each group (i.e., each file), the following steps are performed:\n",
    "     - **Reset Index:**\n",
    "       - The group's index is reset to ensure sequential access.\n",
    "     - **Check Group Length:**\n",
    "       - Only groups with a length equal to or exceeding the `sequence_length` are considered, ensuring that each sequence has the required number of time steps.\n",
    "     - **Sliding Window Approach:**\n",
    "       - A sliding window iterates over the group to extract sequences:\n",
    "         - **Sequence (`seq`):**\n",
    "           - A subset of the group's data spanning `sequence_length` consecutive rows.\n",
    "           - Extracts the specified `features` and converts them to a NumPy array.\n",
    "         - **Label (`y`):**\n",
    "           - The target value at the last time step of the sequence, representing the event or phase to be predicted.\n",
    "       - The extracted `seq` and `y` are appended to `self.sequences` and `self.labels`, respectively.\n",
    "\n",
    "### Length (`__len__`)\n",
    "\n",
    "#### Purpose\n",
    "Returns the total number of sequences available in the dataset.\n",
    "\n",
    "#### Implementation\n",
    "```python\n",
    "def __len__(self):\n",
    "    return len(self.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11d941c9-0b8b-480d-a3bb-3dfd0e90366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Dataset and DataLoader Preparation    #\n",
    "#########################################\n",
    "\n",
    "class SensorDataset(Dataset):\n",
    "    def __init__(self, data, features, target, sequence_length):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "\n",
    "        grouped = data.groupby('filename')\n",
    "        for _, group in grouped:\n",
    "            group = group.reset_index(drop=True)\n",
    "            group_length = len(group)\n",
    "            if group_length >= sequence_length:\n",
    "                # Generate sequences using a sliding window\n",
    "                for i in range(group_length - sequence_length + 1):\n",
    "                    seq = group.iloc[i:i+sequence_length]\n",
    "                    self.sequences.append(seq[self.features].values)\n",
    "                    # Use the event at the last time point as the label\n",
    "                    self.labels.append(seq[self.target].values[-1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.sequences[idx]\n",
    "        y = self.labels[idx]\n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3b7c4-64f5-444c-8a1f-a00d60634bea",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "\n",
    "In this section, we define the `LSTMClassifier` class, a custom neural network model built using PyTorch. This model leverages Long Short-Term Memory (LSTM) layers to effectively capture temporal dependencies in sequential sensor data, making it well-suited for classification tasks based on time-series inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## `LSTMClassifier` Class\n",
    "\n",
    "### Purpose\n",
    "The `LSTMClassifier` is designed to perform classification tasks on sequential data by utilizing LSTM layers to model temporal relationships. It processes input sequences of sensor data and outputs class probabilities corresponding to different phases or events.\n",
    "\n",
    "### Inheritance\n",
    "- **`nn.Module`**: The class inherits from PyTorch's `nn.Module`, enabling integration with PyTorch's model training and evaluation frameworks.\n",
    "\n",
    "### Initialization (`__init__`)\n",
    "\n",
    "#### Parameters\n",
    "- **`input_dim`** (`int`): The number of input features per time step (e.g., number of sensor readings).\n",
    "- **`hidden_dim`** (`int`): The number of features in the hidden state of the LSTM. Determines the capacity of the LSTM to capture patterns.\n",
    "- **`num_layers`** (`int`): The number of stacked LSTM layers. More layers can capture more complex temporal patterns.\n",
    "- **`num_classes`** (`int`): The number of output classes for classification.\n",
    "- **`dropout`** (`float`, optional): The dropout probability for regularization between LSTM layers. Default is `0.5`.\n",
    "\n",
    "#### Components\n",
    "1. **LSTM Layer (`self.lstm`):**\n",
    "   - **Architecture:**\n",
    "     - **Input Size (`input_dim`)**: The dimensionality of the input features.\n",
    "     - **Hidden Size (`hidden_dim`)**: The dimensionality of the hidden state.\n",
    "     - **Number of Layers (`num_layers`)**: Stacked LSTM layers for deeper temporal modeling.\n",
    "     - **Batch First (`batch_first=True`)**: Ensures that the input and output tensors are provided as `(batch, seq, feature)`.\n",
    "     - **Dropout (`dropout=dropout`)**: Applies dropout between LSTM layers to prevent overfitting.\n",
    "   - **Purpose:** Processes the input sequences and captures temporal dependencies across time steps.\n",
    "\n",
    "2. **Fully Connected Layer (`self.fc`):**\n",
    "   - **Architecture:**\n",
    "     - **Input Features (`hidden_dim`)**: Matches the hidden state size of the LSTM.\n",
    "     - **Output Features (`num_classes`)**: Corresponds to the number of target classes.\n",
    "   - **Purpose:** Maps the final hidden state of the LSTM to class scores for classification.\n",
    "\n",
    "#### Implementation\n",
    "```python\n",
    "def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout=0.5):\n",
    "    super(LSTMClassifier, self).__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n",
    "                        batch_first=True, dropout=dropout)\n",
    "    self.fc = nn.Linear(hidden_dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981aac76-7ec4-45b4-abd7-8b1a3fd0731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Model Definition                      #\n",
    "#########################################\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Take output from the last time step\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80a535-2281-415d-acfe-c03ebad87ed3",
   "metadata": {},
   "source": [
    "# Training and Validation Functions\n",
    "\n",
    "This section defines `train_epoch` and `validate_epoch` functions for training and validating the `LSTMClassifier` model.\n",
    "\n",
    "---\n",
    "\n",
    "## `train_epoch`\n",
    "\n",
    "### Purpose\n",
    "Trains the model for one epoch by processing batches, computing loss, performing backpropagation, and updating model weights.\n",
    "\n",
    "### Parameters\n",
    "- **`model`** (`nn.Module`): The neural network model.\n",
    "- **`loader`** (`DataLoader`): DataLoader for training data.\n",
    "- **`criterion`** (`nn.Module`): Loss function.\n",
    "- **`optimizer`** (`torch.optim.Optimizer`): Optimization algorithm.\n",
    "- **`device`** (`torch.device`): Computation device.\n",
    "\n",
    "### Returns\n",
    "- **`epoch_loss`** (`float`): Average loss for the epoch.\n",
    "- **`epoch_acc`** (`float`): Accuracy for the epoch.\n",
    "\n",
    "---\n",
    "\n",
    "## `validate_epoch`\n",
    "\n",
    "### Purpose\n",
    "Evaluates the model on the validation dataset without updating weights.\n",
    "\n",
    "### Parameters\n",
    "- **`model`** (`nn.Module`): The neural network model.\n",
    "- **`loader`** (`DataLoader`): DataLoader for validation data.\n",
    "- **`criterion`** (`nn.Module`): Loss function.\n",
    "- **`device`** (`torch.device`): Computation device.\n",
    "\n",
    "### Returns\n",
    "- **`epoch_loss`** (`float`): Average loss for the epoch.\n",
    "- **`epoch_acc`** (`float`): Accuracy for the epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe0aacd2-1b0a-47e8-aaaf-d671fac21f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Training and Validation Functions     #\n",
    "#########################################\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd6a4e09-34ca-466f-9f3c-aa73aba0fdf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6469, Train Acc: 0.6865, Val Loss: 0.2661, Val Acc: 0.8970\n",
      "Epoch 2/10, Train Loss: 0.2646, Train Acc: 0.8986, Val Loss: 0.2514, Val Acc: 0.9037\n",
      "Epoch 3/10, Train Loss: 0.2550, Train Acc: 0.9013, Val Loss: 0.2467, Val Acc: 0.9028\n",
      "Epoch 4/10, Train Loss: 0.2480, Train Acc: 0.9028, Val Loss: 0.2374, Val Acc: 0.9062\n",
      "Epoch 5/10, Train Loss: 0.2426, Train Acc: 0.9043, Val Loss: 0.2338, Val Acc: 0.9077\n",
      "Epoch 6/10, Train Loss: 0.2392, Train Acc: 0.9052, Val Loss: 0.2282, Val Acc: 0.9072\n",
      "Epoch 7/10, Train Loss: 0.2357, Train Acc: 0.9056, Val Loss: 0.2277, Val Acc: 0.9084\n",
      "Epoch 8/10, Train Loss: 0.2323, Train Acc: 0.9069, Val Loss: 0.2231, Val Acc: 0.9091\n",
      "Epoch 9/10, Train Loss: 0.2303, Train Acc: 0.9073, Val Loss: 0.2241, Val Acc: 0.9095\n",
      "Epoch 10/10, Train Loss: 0.2293, Train Acc: 0.9078, Val Loss: 0.2212, Val Acc: 0.9112\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# Hyperparameter Tuning Configuration   #\n",
    "#########################################\n",
    "\n",
    "data_dir = 'csv_output_phases'  # Replace with your directory\n",
    "raw_data = load_data_from_files(data_dir)\n",
    "data, label_encoder = preprocess_data(raw_data)\n",
    "\n",
    "# Hyperparameters to tune\n",
    "lstm_layers_options = [3]\n",
    "batch_size_options = [128]\n",
    "sequence_length_options = [30]\n",
    "hidden_dim_options = [128]\n",
    "learning_rate_options = [0.001]\n",
    "optimizer_options = ['adam']\n",
    "# Different splits could be tested, but keep at least one split consistent first:\n",
    "data_splits = [\n",
    "    (0.7, 0.15, 0.15),\n",
    "]\n",
    "\n",
    "num_epochs = 10\n",
    "input_dim = len(['gyroscope_x', 'gyroscope_y', 'gyroscope_z',\n",
    "                 'accelerometer_x', 'accelerometer_y', 'accelerometer_z'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "best_val_acc = -1.0\n",
    "best_config = None\n",
    "best_model_state = None\n",
    "\n",
    "for split in data_splits:\n",
    "    train_ratio, val_ratio, test_ratio = split\n",
    "\n",
    "    for seq_length in sequence_length_options:\n",
    "        # Rebuild dataset and splits for each sequence length\n",
    "        dataset = SensorDataset(data, \n",
    "                                features=['gyroscope_x', 'gyroscope_y', 'gyroscope_z',\n",
    "                                          'accelerometer_x', 'accelerometer_y', 'accelerometer_z'],\n",
    "                                target='phase', \n",
    "                                sequence_length=seq_length)\n",
    "\n",
    "        dataset_size = len(dataset)\n",
    "        train_size = int(train_ratio * dataset_size)\n",
    "        val_size = int(val_ratio * dataset_size)\n",
    "        test_size = dataset_size - train_size - val_size\n",
    "\n",
    "        # Set seed for reproducibility in splits\n",
    "        torch.manual_seed(42)\n",
    "        train_dataset, val_dataset, test_dataset = random_split(\n",
    "            dataset, [train_size, val_size, test_size])\n",
    "\n",
    "        for batch_size in batch_size_options:\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "            for lstm_layers in lstm_layers_options:\n",
    "                for hidden_dim in hidden_dim_options:\n",
    "                    for lr in learning_rate_options:\n",
    "                        for opt_name in optimizer_options:\n",
    "                            \n",
    "                            # Initialize model\n",
    "                            model = LSTMClassifier(input_dim, hidden_dim, lstm_layers, num_classes, dropout=0.5).to(device)\n",
    "                            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                            if opt_name == 'adam':\n",
    "                                optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                            else:\n",
    "                                optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "                            # Training loop\n",
    "                            for epoch in range(num_epochs):\n",
    "                                train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "                                val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "                                print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "                                f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "                                f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "                            # Check if this is the best so far\n",
    "                            if val_acc > best_val_acc:\n",
    "                                best_val_acc = val_acc\n",
    "                                best_config = {\n",
    "                                    'train_ratio': train_ratio,\n",
    "                                    'val_ratio': val_ratio,\n",
    "                                    'test_ratio': test_ratio,\n",
    "                                    'sequence_length': seq_length,\n",
    "                                    'batch_size': batch_size,\n",
    "                                    'lstm_layers': lstm_layers,\n",
    "                                    'hidden_dim': hidden_dim,\n",
    "                                    'learning_rate': lr,\n",
    "                                    'optimizer': opt_name\n",
    "                                }\n",
    "                                best_model_state = model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7e316-c4f9-44b5-aa16-cdf274394c87",
   "metadata": {},
   "source": [
    "# Evaluate Best Model on Test Set\n",
    "\n",
    "This section assesses the performance of the best model configuration on the test dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Display Best Metrics:**\n",
    "   - Prints the highest validation accuracy achieved and the corresponding configuration parameters.\n",
    "\n",
    "2. **Rebuild Dataset and Model:**\n",
    "   - Extracts the best configuration settings such as sequence length, batch size, LSTM layers, hidden dimensions, learning rate, optimizer type, and dataset split ratios.\n",
    "   - Creates a `SensorDataset` using the optimal sequence length.\n",
    "   - Splits the dataset into training, validation, and test sets based on the best ratios.\n",
    "\n",
    "3. **Initialize DataLoaders:**\n",
    "   - Sets up DataLoaders for training, validation, and testing with the optimal batch size and shuffling where appropriate.\n",
    "\n",
    "4. **Recreate and Load the Model:**\n",
    "   - Instantiates the `LSTMClassifier` with the best hyperparameters.\n",
    "   - Loads the saved state of the best-performing model.\n",
    "   - Sets the model to evaluation mode to prepare for testing.\n",
    "\n",
    "5. **Evaluate on Test Set:**\n",
    "   - Iterates through the test DataLoader, making predictions with the model.\n",
    "   - Collects all predictions and true labels.\n",
    "   - Generates and prints a classification report detailing precision, recall, f1-score, and support for each class.\n",
    "\n",
    "---\n",
    "\n",
    "By executing these steps, we validate the model's effectiveness on unseen data, ensuring its ability to generalize beyond the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a8c83ee-58b5-4db7-a5c1-a5065a22d591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy:  0.9112463662790697\n",
      "Best Configuration:  {'train_ratio': 0.7, 'val_ratio': 0.15, 'test_ratio': 0.15, 'sequence_length': 30, 'batch_size': 128, 'lstm_layers': 3, 'hidden_dim': 128, 'learning_rate': 0.001, 'optimizer': 'adam'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    no event       0.84      0.21      0.34       265\n",
      " heel strike       0.00      0.00      0.00        33\n",
      "   foot flat       0.90      0.94      0.92     22740\n",
      "    heel off       0.82      0.77      0.80     14536\n",
      "     toe off       0.96      0.97      0.96     28474\n",
      "\n",
      "    accuracy                           0.91     66048\n",
      "   macro avg       0.70      0.58      0.60     66048\n",
      "weighted avg       0.91      0.91      0.91     66048\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ap/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/ap/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/ap/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# Evaluate Best Model on Test Set       #\n",
    "#########################################\n",
    "\n",
    "print(\"Best Validation Accuracy: \", best_val_acc)\n",
    "print(\"Best Configuration: \", best_config)\n",
    "\n",
    "# Rebuild the dataset and model with best config\n",
    "sequence_length = best_config['sequence_length']\n",
    "batch_size = best_config['batch_size']\n",
    "lstm_layers = best_config['lstm_layers']\n",
    "hidden_dim = best_config['hidden_dim']\n",
    "lr = best_config['learning_rate']\n",
    "opt_name = best_config['optimizer']\n",
    "train_ratio = best_config['train_ratio']\n",
    "val_ratio = best_config['val_ratio']\n",
    "test_ratio = best_config['test_ratio']\n",
    "\n",
    "# Recreate dataset with the best sequence length\n",
    "best_dataset = SensorDataset(data,\n",
    "                            features=['gyroscope_x', 'gyroscope_y', 'gyroscope_z',\n",
    "                                      'accelerometer_x', 'accelerometer_y', 'accelerometer_z'],\n",
    "                            target='phase', \n",
    "                            sequence_length=sequence_length)\n",
    "\n",
    "dataset_size = len(best_dataset)\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "val_size = int(val_ratio * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    best_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# Recreate model and load best state\n",
    "final_model = LSTMClassifier(input_dim, hidden_dim, lstm_layers, num_classes, dropout=0.5).to(device)\n",
    "final_model.load_state_dict(best_model_state)\n",
    "final_model.eval()\n",
    "\n",
    "# Evaluate on test set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        outputs = final_model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "class_names = ['no event', 'heel strike', 'foot flat', 'heel off', 'toe off']\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15007905-137f-487b-ad80-7f81dba7be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to best_lstm_model_phases.pth\n"
     ]
    }
   ],
   "source": [
    "# After training is complete\n",
    "model_save_path = 'best_lstm_model_phases.pth'\n",
    "torch.save(final_model.state_dict(), model_save_path)\n",
    "print(f\"Model weights saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcf589d-325e-444a-87e1-9b49bc41ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the model architecture\n",
    "# loaded_model = LSTMClassifier(input_dim, hidden_dim, num_layers, num_classes)\n",
    "# loaded_model.to(device)\n",
    "\n",
    "# # Load the saved weights\n",
    "# model_load_path = 'lstm_model_phases.pth'\n",
    "# loaded_model.load_state_dict(torch.load(model_load_path))\n",
    "# print(f\"Model weights loaded from {model_load_path}\")\n",
    "\n",
    "# # Set the model to evaluation mode\n",
    "# loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea9bb9-2077-4921-8f8a-ee13077c38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate on the test set\n",
    "# test_loss, test_acc = validate(loaded_model, test_loader, criterion, device)\n",
    "# print(f'Loaded Model Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf705a52-c52a-4cc3-99e4-3ce324a235db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# import numpy as np\n",
    "\n",
    "# # Collect predictions and true labels\n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for X_batch, y_batch in test_loader:\n",
    "#         X_batch = X_batch.to(device)\n",
    "#         y_batch = y_batch.to(device)\n",
    "#         outputs = loaded_model(X_batch)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         all_preds.extend(predicted.cpu().numpy())\n",
    "#         all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "# # Define all possible labels\n",
    "# labels = range(num_classes)  # Ensure num_classes is set correctly (should be 5)\n",
    "\n",
    "# # Manually define class names\n",
    "# class_names = ['no event', 'heel strike', 'foot flat', 'heel off', 'toe off']\n",
    "\n",
    "# # Generate the classification report\n",
    "# print(classification_report(all_labels, all_preds, labels=labels, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44ff83-5e7c-4a92-be1c-ceec9f50043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Define phase labels\n",
    "# phase_labels = {\n",
    "#     0: 'no event',\n",
    "#     1: 'heel strike',\n",
    "#     2: 'foot flat',\n",
    "#     3: 'heel off',\n",
    "#     4: 'toe off'\n",
    "# }\n",
    "# # Create subplots for each feature\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(24, 12))\n",
    "\n",
    "# # Plot each feature over time for the first collection_id with different colors for each phase\n",
    "# features = ['gyroscope_x', 'gyroscope_y', 'gyroscope_z', 'accelerometer_x', 'accelerometer_y', 'accelerometer_z']\n",
    "# for i, feature in enumerate(features):\n",
    "#     ax = axes[i // 3, i % 3]\n",
    "#     for phase in phases:\n",
    "#         phase_data = first_collection_data[first_collection_data['phase'] == phase]\n",
    "#         ax.plot(phase_data['elapsed_time'], phase_data[feature], label=f'{feature} - {phase_labels[phase]}')\n",
    "    \n",
    "#     ax.set_xlabel('Elapsed Time (seconds)')\n",
    "#     ax.set_ylabel(feature)\n",
    "#     ax.set_title(f'{feature} over Time for Collection ID: {first_collection_id}')\n",
    "#     ax.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
